{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Copy of M4_MP1_NB_Traffic_sign_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JainAnki/ADSMI-Notebooks/blob/main/Copy_of_M4_MP1_NB_Traffic_sign_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "powered-thong"
      },
      "source": [
        "# Applied Data Science and Machine Intelligence\n",
        "## A program by IIT Madras and TalentSprint\n",
        "### Mini Project: Image classification using MLP and CNN\n",
        "\n"
      ],
      "id": "powered-thong"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "maritime-miami"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* load and extract features of images\n",
        "\n",
        "* implement the Multi-Layer perceptron to classify images\n",
        "\n",
        "* implement CNN using keras"
      ],
      "id": "nljJR6CwfZN_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29152de7"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Traffic sign recognition is a challenging, real-world problem relevant for AI based transportation systems. Traffic signs show a wide range of variations between classes in terms of color, shape, and the presence of pictograms or text. However, there exist subsets of\n",
        "classes (e.g., speed limit signs) that are very similar to each other. Further, the classifier\n",
        "has to be robust against large variations in visual appearances due to changes in illumination, partial\n",
        "occlusions, rotations, weather conditions etc. Using a comprehensive traffic sign detection dataset, here we will perform classification of traffic signs, train and evaluate the different models and compare to the performance of MLPs."
      ],
      "id": "29152de7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58facc94"
      },
      "source": [
        "![img](https://paperswithcode.com/media/datasets/GTSRB-0000000633-9ce3c5f6_Dki5Rsf.jpg)"
      ],
      "id": "58facc94"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "surprising-uruguay"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The data for this mini-project is from the German Traffic Sign Detection Benchmark [GTSDB](https://benchmark.ini.rub.de/gtsdb_dataset.html). This archive contains the training set used during the IJCNN 2013 competition. \n",
        "\n",
        "The German Traffic Sign Detection Benchmark is a single-image detection assessment for researchers with interest in the field of computer vision, pattern recognition and image-based driver assistance. It is introduced on the IEEE International Joint Conference on Neural Networks 2013. \n",
        "\n",
        "It features ...\n",
        "\n",
        "* The main archive FullIJCNN2013.zip includes the images (1360 x 800 pixels) in PPM format, the image sections containing only the traffic signs\n",
        "* A file in CSV format with the ground truth\n",
        "* A ReadMe.txt with more details."
      ],
      "id": "surprising-uruguay"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ],
      "id": "ih-oasWmdZul"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To build and improve upon a machine learning model for the classification of images and achieve a high accuracy final model."
      ],
      "id": "qfWGmjNHdZul"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operating-latter"
      },
      "source": [
        "## Grading = 10 Points"
      ],
      "id": "operating-latter"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "812a816f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de570814-aa09-4ec4-909b-06e8653413df"
      },
      "source": [
        "#@title Download the data\n",
        "!wget -qq https://sid.erda.dk/public/archives/ff17dc924eba88d5d01a807357d6614c/FullIJCNN2013.zip\n",
        "!unzip -qq FullIJCNN2013.zip"
      ],
      "id": "812a816f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace FullIJCNN2013/00000.ppm? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace FullIJCNN2013/00001.ppm? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace FullIJCNN2013/00001.ppm? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "N\n",
            "n\n",
            "n\n",
            "A\n",
            "Na\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "### Import Required packages"
      ],
      "id": "abstract-stocks"
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "A_UsJjgb50l-"
      },
      "id": "A_UsJjgb50l-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advisory-knowing"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from skimage.io import imread, imshow\n",
        "from sklearn import preprocessing\n",
        "import os, glob\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Keras\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "import cv2\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "import sklearn"
      ],
      "id": "advisory-knowing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f0UsVhg3Cl5",
        "outputId": "cb26da76-cc54-4f98-dd56-f292d19cfd0a"
      },
      "id": "8f0UsVhg3Cl5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp4bF_GJdZuo"
      },
      "source": [
        "###**Excercise 1**\n",
        "\n",
        "### Data Loading and Feature Extraction (1 points)\n",
        "\n",
        "#### Get the features and labels of data\n",
        "\n",
        "* Extract the features of the images\n",
        "* Extract labels of the images\n",
        "* Resize the images to (30, 30) and convert to numpy 1-D array\n",
        "\n",
        "   Hint: [Link](https://machinelearningmastery.com/how-to-load-and-manipulate-images-for-deep-learning-in-python-with-pil-pillow/)"
      ],
      "id": "gp4bF_GJdZuo"
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "dir_path = r'/content/FullIJCNN2013/'\n",
        "\n",
        "for i in range(10):\n",
        "  os.rename(dir_path+'0'+str(i),dir_path+str(i))\n",
        "\n",
        "# load the image\n",
        "#folders = os.listdir('../GTSRB/')\n",
        "train_images = []\n",
        "dir_path = r'/content/FullIJCNN2013/'\n",
        "feats = pd.DataFrame([[0]*900])\n",
        "class_= []\n",
        "sample_images = []\n",
        "#print(len([entry for entry in os.listdir(f'FullIJCNN2013/{i}') if os.path.isfile(os.path.join(f'FullIJCNN2013/{i}', entry))]))\n",
        "for i in range(43):\n",
        "    no_of_files = (len([entry for entry in os.listdir(dir_path+str(i)) if os.path.isfile(os.path.join(dir_path, entry))]))\n",
        "    for j in range(no_of_files):\n",
        "      image_id = str(j).rjust(5,'0')\n",
        "      img = Image.open(dir_path+str(i)+'/'+str(image_id)+'.ppm').convert('L')\n",
        "      img = img.resize((30,30))\n",
        "      if j==0:\n",
        "        sample_images.append(img)\n",
        "      arr = np.array(img)\n",
        "\n",
        "      # record the original shape\n",
        "      shape = arr.shape\n",
        "\n",
        "      # make a 1-dimensional view of arr\n",
        "      flat_arr = arr.ravel()\n",
        "      feats.loc[len(feats)] = list(flat_arr)\n",
        "      class_.append(i) \n",
        "      # convert it to a matrix\n",
        "      vector = np.matrix(flat_arr)\n",
        "\n",
        "      # reform a numpy array of the original shape\n",
        "      arr2 = np.asarray(vector).reshape(shape)\n",
        "      # make a PIL image\n",
        "      #img2 = Image.fromarray(arr2, 'L')\n",
        "      #img2\n",
        "      train_images.append(arr2)\n",
        "      #oneD_arr.append(flat_arr)\n",
        "      #print(img.size)\n",
        "labels = pd.DataFrame([class_]).T  \n",
        "feats = feats.iloc[1: , :]\n",
        "\n",
        "'''\n",
        "print(image1.dtype)\n",
        "print(image1.shape)\n",
        "# display the array of pixels as an image\n",
        "pyplot.imshow(image1)\n",
        "pyplot.show()\n",
        "\n",
        "# convert image to numpy array\n",
        "data = asarray(image)\n",
        "# summarize shape\n",
        "print(data.shape)\n",
        "# create Pillow image\n",
        "image2 = Image.fromarray(data)\n",
        "# summarize image details\n",
        "'''\n",
        "'''\n",
        "print(img.format)\n",
        "print(img.mode)\n",
        "print(img.size)\n",
        "label_df = pd.read_csv(f'FullIJCNN2013/gt.txt',names=['file','x1','y1','x2','y2','class'],delimiter = ';')\n",
        "train_feats = label_df[['x1','y1','x2','y2']].copy()\n",
        "train_labels = label_df[['class']].copy()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "7C5SoohW4PQ2",
        "outputId": "ab093790-51f1-49c7-8bbf-13112aa76b81"
      },
      "id": "7C5SoohW4PQ2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-20fb1fe3df49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 39] Directory not empty: '/content/FullIJCNN2013/00' -> '/content/FullIJCNN2013/0'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc5c2362"
      },
      "source": [
        "'''\n",
        "#folders = os.listdir('../GTSRB/')\n",
        "train_images = []\n",
        "train_labels = []\n",
        "first_file = 0\n",
        "for i in range(600):\n",
        "    image_id = str(i).rjust(5,'0')\n",
        "    image = cv2.imread(f'FullIJCNN2013/{image_id}.ppm')\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (30, 30))\n",
        "    train_images.append(image)\n",
        "print(len(train_images))\n",
        "data = asarray(image)\n",
        "\n",
        "#Extract largest dimension for each image\n",
        "size_list = []\n",
        "for i in train_images:\n",
        "    size_list.append(max(i.shape[0],i.shape[1]))\n",
        "\n",
        "label_df = pd.read_csv(f'FullIJCNN2013/gt.txt',names=['file','x1','y1','x2','y2','class'],delimiter = ';')\n",
        "'''"
      ],
      "id": "fc5c2362",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUY3yNrdaABY"
      },
      "source": [
        "###**Excercise 2**\n",
        "### Data Exploration and Preprocessing ( 2 points)"
      ],
      "id": "NUY3yNrdaABY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ca63666"
      },
      "source": [
        "#### Plot the sample image of each class\n",
        "\n",
        "Hint: plt.subplot"
      ],
      "id": "9ca63666"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c414e14e"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
        "plt.rcParams[\"figure.autolayout\"] = False\n",
        "fig, axs = plt.subplots(5,9)\n",
        "fig.suptitle('Sample image of each classs')\n",
        "img_no = 0\n",
        "for i in range(5):\n",
        "  for j in range(9):\n",
        "    axs[i,j].imshow(sample_images[img_no])\n",
        "    img_no+=1\n",
        "    if img_no>42:\n",
        "      break"
      ],
      "id": "c414e14e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a2rqCM-sIbY"
      },
      "source": [
        "#### Plot the distribution of Classes"
      ],
      "id": "8a2rqCM-sIbY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwWKGQMFsIDP"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "plt.bar(unique, counts, 1)\n",
        "plt.title('Class Frequency')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "id": "nwWKGQMFsIDP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37b23a0b"
      },
      "source": [
        "#### Normalize the features\n",
        "\n",
        "For most image data, the pixel values are integers with values between 0 and 255.\n",
        "\n",
        "Neural networks process inputs using small weight values, and inputs with large integer values can disrupt or slow down the learning process. As such it is good practice to normalize the pixel values.\n",
        "\n",
        "Hint: sklearn.preprocessing.normalize"
      ],
      "id": "37b23a0b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82239736"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "features = sklearn.preprocessing.normalize(feats,norm = 'l2')\n",
        "type(features)\n",
        "feat = pd.DataFrame(features)\n",
        "feat"
      ],
      "id": "82239736",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MRpw70ikBwA"
      },
      "source": [
        "###**Excercise 3**\n",
        "### Train the MLP classifier on features (3 points)\n",
        "\n",
        "* Split the data into train and test\n",
        "\n",
        "* Train the MLP classifier with different parameters\n",
        "\n",
        "* Get the accuracy score and performance metrics"
      ],
      "id": "_MRpw70ikBwA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af9cd34e"
      },
      "source": [
        "\n",
        "\n",
        "* Define the keras model and initialize the layers\n",
        "  - Ensure the input layer has the right number of input features. This can be specified when creating the first layer with the input_dim argument.\n",
        "* Compile the model\n",
        "  - Specify the loss function (to evaluate a set of weights), the optimizer (is used to search through different weights for the network) and any optional metrics to collect and report during training.\n",
        "* Fit and Evaluate the model\n",
        "  - Fit the data by specifying epochs and evaluate the model"
      ],
      "id": "af9cd34e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbfQyg61jEbv"
      },
      "source": [
        "#MLP\n",
        "X_train, X_test, y_train, y_test = train_test_split(feat, labels, stratify=labels, random_state=1)\n",
        "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
        "clf.predict_proba(X_test),clf.predict(X_test),clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "qbfQyg61jEbv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Build the architecture\n",
        "# YOUR CODE HERE\n",
        "model = Sequential([\n",
        "\t\n",
        "\t# reshape 28 row * 28 column data to 28*28 rows\n",
        "\tkeras.layers.Flatten(input_dim=900),\n",
        "\t\n",
        "\t# dense layer 1\n",
        "\tkeras.layers.Dense(256, activation='sigmoid'),\n",
        "\t\n",
        "\t# dense layer 2\n",
        "\tkeras.layers.Dense(128, activation='sigmoid'),\n",
        "\t\n",
        "\t# output laye,r\n",
        "\tkeras.layers.Dense(43, activation='sigmoid'),\n",
        "])\n"
      ],
      "metadata": {
        "id": "DedAaImPU9pE"
      },
      "id": "DedAaImPU9pE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xy1y2eXjEby"
      },
      "source": [
        "# Step 2 - Compile the model\n",
        "# YOUR CODE HERE\n",
        "model.compile(optimizer='adam',\n",
        "\t\t\tloss='sparse_categorical_crossentropy',\n",
        "\t\t\tmetrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5xy1y2eXjEby"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_LTVFlfjEb1"
      },
      "source": [
        "# Step 3 - Fit and Evaluate the model\n",
        "# YOUR CODE HERE\n",
        "#y_train = keras.utils.to_categorical(y_train, 43)\n",
        "#y_test = keras.utils.to_categorical(y_test, 43)\n",
        "model.fit(X_train, y_train, epochs=1000,\n",
        "\t\tbatch_size=200,\n",
        "\t\tvalidation_split=0.3)\n",
        "Pred = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Dense fully connected network results on the test data - Baseline \")\n",
        "print(\" \")\n",
        "print(\"%s- %.2f\" % (model.metrics_names[0], Pred[0]))\n",
        "print(\"%s- %.2f\" % (model.metrics_names[1], Pred[1]))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2_LTVFlfjEb1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "911d0a39"
      },
      "source": [
        "#### Try the different algorithms and compare the results with MLP classifier"
      ],
      "id": "911d0a39"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-FSvc27_64O"
      },
      "source": [
        "###**Excercise 4**\n",
        "### Train a CNN classifier on images (4 points)\n",
        "\n",
        "* Split the data into train and test\n",
        "\n",
        "* Train the CNN with 2D convolution and Maxpooling layers\n",
        "\n",
        "* Get the accuracy score on train and test sets "
      ],
      "id": "G-FSvc27_64O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Guawf8U_64S"
      },
      "source": [
        "\n",
        "\n",
        "* Define the keras model and initialize the layers\n",
        "  - Ensure the input layer is specified with correct image size as input. This can be specified when creating the first layer with the input_shape argument.\n",
        "* Speicify number of filters Kernel size, Pool size and activation function\n",
        "  - filters,kernel_size and activation arguments of Conv2D layer can be used\n",
        "  - pool_size argument of MaxPool2D can be used to set Pool size\n",
        "* Compile the model\n",
        "  - Specify the loss function (to evaluate a set of weights), the optimizer (is used to search through different weights for the network) and any optional metrics to collect and report during training.\n",
        "* Fit and Evaluate the model\n",
        "  - Fit the data by specifying epochs and evaluate the model"
      ],
      "id": "0Guawf8U_64S"
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.DataFrame(train_images)\n",
        "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(train_images, class_, stratify=labels, random_state=1)\n"
      ],
      "metadata": {
        "id": "C1S5SyUreeKZ"
      },
      "id": "C1S5SyUreeKZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers import Dropout # new!\n",
        "from keras.layers import BatchNormalization # new!\n",
        "from keras import regularizers # new! \n",
        "from keras.layers import Flatten, Conv2D, MaxPooling2D # new!\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "lSFnt30Hllfg"
      },
      "id": "lSFnt30Hllfg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eMm6CWljLnm"
      },
      "source": [
        "# Step 1 - Build the architecture\n",
        "model_conv = Sequential()\n",
        "## If You preprocessed with gray scaling and local histogram equivalization then input_shape = (32,32,1) else (32,32,3)\n",
        "model_conv.add(Conv2D(32, kernel_size=(3, 3),activation='relu', input_shape=(30, 30, 1)))\n",
        "model_conv.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model_conv.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model_conv.add(Dropout(0.25))\n",
        "model_conv.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model_conv.add(Dropout(0.5))\n",
        "model_conv.add(Flatten())\n",
        "model_conv.add(Dense(128, activation='relu'))\n",
        "model_conv.add(Dropout(0.5))\n",
        "model_conv.add(Dense(43, activation='softmax'))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "-eMm6CWljLnm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2R8QP0EjLnn"
      },
      "source": [
        "# Step 2 - Compile the model\n",
        "# YOUR CODE HERE\n",
        "model_conv.summary()\n",
        "model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "r2R8QP0EjLnn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugvAPL9YjLno"
      },
      "source": [
        "## Step 3 - Fit and Evaluate the model\n",
        "# YOUR CODE HERE\n",
        "X_train_cnn = np.asarray(X_train_cnn).astype(np.float32)\n",
        "X_test_cnn = np.asarray(X_test_cnn).astype(np.float32)\n",
        "y_train_cnn = keras.utils.to_categorical(y_train_cnn, 43)\n",
        "y_test_cnn = keras.utils.to_categorical(y_test_cnn, 43)\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "model_conv.fit(X_train_cnn, y_train_cnn, epochs=200,\n",
        "\t\tbatch_size=200,\n",
        "\t\tvalidation_split=0.3)\n",
        "Pred_conv = model_conv.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
        "print(\"%s- %.2f\" % (model_conv.metrics_names[0], Pred_conv[0]))\n",
        "print(\"%s- %.2f\" % (model_conv.metrics_names[1], Pred_conv[1]))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ugvAPL9YjLno"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAHzeVx_tImO"
      },
      "source": [
        "#### Experiment using Dropout, Regularization and Batch Normalization"
      ],
      "id": "IAHzeVx_tImO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w34gbejXvLUs"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "w34gbejXvLUs",
      "execution_count": null,
      "outputs": []
    }
  ]
}